{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T15:46:12.078818Z","iopub.status.busy":"2024-10-04T15:46:12.078347Z","iopub.status.idle":"2024-10-04T15:46:14.967497Z","shell.execute_reply":"2024-10-04T15:46:14.966528Z","shell.execute_reply.started":"2024-10-04T15:46:12.078780Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Implementation of ProGAN generator and discriminator with the key\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from math import log2\n","\n","import torch\n","import random\n","import numpy as np\n","import os\n","import torchvision\n","from torchvision.utils import save_image\n","from scipy.stats import truncnorm\n","\n","\n","\n","#Factors is used in Discrmininator and Generator for how muchthe channels should be multiplied and expanded for each layer,\n","factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n","\n","\n","class WSConv2d(nn.Module):\n","\n","    def __init__(\n","        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n","    ):\n","        super(WSConv2d, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n","        self.bias = self.conv.bias\n","        self.conv.bias = None\n","\n","        # initialize conv layer\n","        nn.init.normal_(self.conv.weight)\n","        nn.init.zeros_(self.bias)\n","\n","    def forward(self, x):\n","        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n","\n","\n","class PixelNorm(nn.Module):\n","    def __init__(self):\n","        super(PixelNorm, self).__init__()\n","        self.epsilon = 1e-8\n","\n","    def forward(self, x):\n","        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","        super(ConvBlock, self).__init__()\n","        self.use_pn = use_pixelnorm\n","        self.conv1 = WSConv2d(in_channels, out_channels)\n","        self.conv2 = WSConv2d(out_channels, out_channels)\n","        self.leaky = nn.LeakyReLU(0.2)\n","        self.pn = PixelNorm()\n","\n","    def forward(self, x):\n","        x = self.leaky(self.conv1(x))\n","        x = self.pn(x) if self.use_pn else x\n","        x = self.leaky(self.conv2(x))\n","        x = self.pn(x) if self.use_pn else x\n","        return x\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Generator, self).__init__()\n","\n","        # initial takes 1x1 -> 4x4\n","        self.initial = nn.Sequential(\n","            PixelNorm(),\n","            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(0.2),\n","            PixelNorm(),\n","        )\n","\n","        self.initial_rgb = WSConv2d(\n","            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.prog_blocks, self.rgb_layers = (\n","            nn.ModuleList([]),\n","            nn.ModuleList([self.initial_rgb]),\n","        )\n","\n","        for i in range(\n","            len(factors) - 1\n","        ):  # -1 to prevent index error because of factors[i+1]\n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i + 1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","            self.rgb_layers.append(\n","                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","    def fade_in(self, alpha, upscaled, generated):\n","        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n","\n","    def forward(self, x, alpha, steps):\n","        out = self.initial(x)\n","\n","        if steps == 0:\n","            return self.initial_rgb(out)\n","\n","        for step in range(steps):\n","            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n","            out = self.prog_blocks[step](upscaled)\n","        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n","        final_out = self.rgb_layers[steps](out)\n","        return self.fade_in(alpha, final_upscaled, final_out)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Discriminator, self).__init__()\n","        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n","        self.leaky = nn.LeakyReLU(0.2)\n","        for i in range(len(factors) - 1, 0, -1):\n","            conv_in = int(in_channels * factors[i])\n","            conv_out = int(in_channels * factors[i - 1])\n","            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n","            self.rgb_layers.append(\n","                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","        self.initial_rgb = WSConv2d(\n","            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.rgb_layers.append(self.initial_rgb)\n","        self.avg_pool = nn.AvgPool2d(\n","            kernel_size=2, stride=2\n","        )  # down sampling using avg pool\n","\n","        # this is the block for 4x4 input size\n","        self.final_block = nn.Sequential(\n","            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(\n","                in_channels, 1, kernel_size=1, padding=0, stride=1\n","            ),  \n","        )\n","\n","    def fade_in(self, alpha, downscaled, out):\n","        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n","        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n","        return alpha * out + (1 - alpha) * downscaled\n","\n","    def minibatch_std(self, x):\n","        batch_statistics = (\n","            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n","        )\n"," \n","        return torch.cat([x, batch_statistics], dim=1)\n","\n","    def forward(self, x, alpha, steps):\n"," \n","        cur_step = len(self.prog_blocks) - steps\n","        out = self.leaky(self.rgb_layers[cur_step](x))\n","\n","        if steps == 0:  # i.e, image is 4x4\n","            out = self.minibatch_std(out)\n","            return self.final_block(out).view(out.shape[0], -1)\n","\n","        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n","        out = self.avg_pool(self.prog_blocks[cur_step](out))\n","\n","  \n","        out = self.fade_in(alpha, downscaled, out)\n","\n","        for step in range(cur_step + 1, len(self.prog_blocks)):\n","            out = self.prog_blocks[step](out)\n","            out = self.avg_pool(out)\n","\n","        out = self.minibatch_std(out)\n","        return self.final_block(out).view(out.shape[0], -1)\n","\n","\n","Z_DIM = 256\n","IN_CHANNELS = 256\n","gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n","critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n","\n","for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n","    num_steps = int(log2(img_size / 4))\n","    x = torch.randn((1, Z_DIM, 1, 1))\n","    z = gen(x, 0.5, steps=num_steps)\n","    assert z.shape == (1, 3, img_size, img_size)\n","    out = critic(z, alpha=0.5, steps=num_steps)\n","    assert out.shape == (1, 1)\n","    print(f\"Success! At img size: {img_size}\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T15:46:14.969733Z","iopub.status.busy":"2024-10-04T15:46:14.969333Z","iopub.status.idle":"2024-10-04T15:46:15.327080Z","shell.execute_reply":"2024-10-04T15:46:15.326267Z","shell.execute_reply.started":"2024-10-04T15:46:14.969691Z"},"trusted":true},"outputs":[],"source":["class configuration():\n","    def __init__(self):\n","        self.START_TRAIN_AT_IMG_SIZE = 4\n","        self.DATASET = '/kaggle/input/multi-class-mim'\n","        self.CHECKPOINT_GEN = \"generator.pth\"\n","        self.CHECKPOINT_CRITIC = \"critic.pth\"\n","        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.SAVE_MODEL = True\n","        self.LOAD_MODEL = False\n","        self.LEARNING_RATE = 1e-3\n","        self.BATCH_SIZES = [32, 32, 32, 32, 32]\n","        self.CHANNELS_IMG = 3\n","        self.Z_DIM = 256  # should be 512 in original paper\n","        self.IN_CHANNELS = 256  # should be 512 in original paper\n","        self.CRITIC_ITERATIONS = 1\n","        self.LAMBDA_GP = 10\n","        self.PROGRESSIVE_EPOCHS = [30] * len(self.BATCH_SIZES)\n","        self.FIXED_NOISE = torch.randn(8, self.Z_DIM, 1, 1).to(self.DEVICE)\n","        self.NUM_WORKERS = 4\n","config=configuration()"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T15:46:20.058306Z","iopub.status.busy":"2024-10-04T15:46:20.057913Z","iopub.status.idle":"2024-10-04T15:46:21.800688Z","shell.execute_reply":"2024-10-04T15:46:21.799619Z","shell.execute_reply.started":"2024-10-04T15:46:20.058270Z"},"trusted":true},"outputs":[],"source":["\n","# Print losses occasionally and print to tensorboard\n","def plot_to_tensorboard(\n","    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n","):\n","    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n","\n","    with torch.no_grad():\n","        # take out (up to) 8 examples to plot\n","        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n","        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n","        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n","        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n","\n","\n","def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def generate_examples(gen, steps, truncation=0.7, n=100):\n","    \"\"\"\n","    Tried using truncation trick here but not sure it actually helped anything, you can\n","    remove it if you like and just sample from torch.randn\n","    \"\"\"\n","    gen.eval()\n","    alpha = 1.0\n","    for i in range(n):\n","        with torch.no_grad():\n","            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, config.Z_DIM, 1, 1)), device=config.DEVICE, dtype=torch.float32)\n","            img = gen(noise, alpha, steps)\n","            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n","    gen.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T15:46:27.251154Z","iopub.status.busy":"2024-10-04T15:46:27.250284Z","iopub.status.idle":"2024-10-04T16:02:56.486544Z","shell.execute_reply":"2024-10-04T16:02:56.485301Z","shell.execute_reply.started":"2024-10-04T15:46:27.251110Z"},"trusted":true},"outputs":[],"source":["\"\"\" Training of ProGAN using WGAN-GP loss\"\"\"\n","\n","import torch\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from math import log2\n","from tqdm import tqdm\n","\n","torch.backends.cudnn.benchmarks = True\n","\n","\n","def get_loader(image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.Normalize(\n","                [0.5 for _ in range(config.CHANNELS_IMG)],\n","                [0.5 for _ in range(config.CHANNELS_IMG)],\n","            ),\n","        ]\n","    )\n","    batch_size = config.BATCH_SIZES[int(log2(image_size / 4))]\n","    dataset = datasets.ImageFolder(root=config.DATASET, transform=transform)\n","    print(len(dataset))\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=config.NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    return loader, dataset\n","\n","\n","def train_fn(\n","    critic,\n","    gen,\n","    loader,\n","    dataset,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","    tensorboard_step,\n","    writer,\n","    scaler_gen,\n","    scaler_critic,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(config.DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n","        # which is equivalent to minimizing the negative of the expression\n","        noise = torch.randn(cur_batch_size, config.Z_DIM, 1, 1).to(config.DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            fake = gen(noise, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=config.DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + config.LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()\n","\n","        # Update alpha and ensure less than 1\n","        alpha += cur_batch_size / (\n","            (config.PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n","        )\n","        alpha = min(alpha, 1)\n","\n","        if batch_idx % 500 == 0:\n","            with torch.no_grad():\n","                fixed_fakes = gen(config.FIXED_NOISE, alpha, step) * 0.5 + 0.5\n","            plot_to_tensorboard(\n","                writer,\n","                loss_critic.item(),\n","                loss_gen.item(),\n","                real.detach(),\n","                fixed_fakes.detach(),\n","                tensorboard_step,\n","            )\n","            tensorboard_step += 1\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            loss_critic=loss_critic.item(),\n","        )\n","\n","    return tensorboard_step, alpha\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","\n","    \n","    gen = Generator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n","    critic = Discriminator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n","    \n","\n","    # initialize optimizers and scalers\n","    opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n","    opt_critic = optim.Adam(\n","        critic.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99)\n","    )\n","    scaler_critic = torch.cuda.amp.GradScaler()\n","    scaler_gen = torch.cuda.amp.GradScaler()\n","\n","    # for tensorboard plotting\n","    writer = SummaryWriter(f\"logs/gan1\")\n","\n","    if config.LOAD_MODEL:\n","        load_checkpoint(\n","            config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            config.CHECKPOINT_CRITIC, critic, opt_critic, config.LEARNING_RATE,\n","        )\n","\n","    gen.train()\n","    critic.train()\n","\n","    tensorboard_step = 0\n","    # start at step that corresponds to img size that we set in config\n","    step = int(log2(config.START_TRAIN_AT_IMG_SIZE / 4))\n","    for num_epochs in config.PROGRESSIVE_EPOCHS[step:]:\n","        alpha = 1e-5  # start with very low alpha\n","        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n","        print(f\"Current image size: {4 * 2 ** step}\")\n","\n","        for epoch in range(num_epochs):\n","            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","            tensorboard_step, alpha = train_fn(\n","                critic,\n","                gen,\n","                loader,\n","                dataset,\n","                step,\n","                alpha,\n","                opt_critic,\n","                opt_gen,\n","                tensorboard_step,\n","                writer,\n","                scaler_gen,\n","                scaler_critic,\n","            )\n","\n","            if config.SAVE_MODEL:\n","                save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n","                save_checkpoint(critic, opt_critic, filename=config.CHECKPOINT_CRITIC)\n","\n","        step += 1  # progress to the next img size\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5637028,"sourceId":9308286,"sourceType":"datasetVersion"},{"datasetId":5758735,"sourceId":9471256,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}

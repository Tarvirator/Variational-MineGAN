{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import os\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","from math import log2\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:38:59.227697Z","iopub.status.busy":"2024-10-04T06:38:59.227265Z","iopub.status.idle":"2024-10-04T06:39:03.282405Z","shell.execute_reply":"2024-10-04T06:39:03.281381Z","shell.execute_reply.started":"2024-10-04T06:38:59.227662Z"},"trusted":true},"outputs":[],"source":["\n","factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n","\n","\n","class WSConv2d(nn.Module):\n","\n","\n","    def __init__(\n","        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n","    ):\n","        super(WSConv2d, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n","        self.bias = self.conv.bias\n","        self.conv.bias = None\n","\n","        # initialize conv layer\n","        nn.init.normal_(self.conv.weight)\n","        nn.init.zeros_(self.bias)\n","\n","    def forward(self, x):\n","        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n","\n","class WSLinear(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(WSLinear,self).__init__()\n","        self.lin=nn.Linear(in_channels, out_channels)\n","        self.bias=self.lin.bias\n","        self.lin.bias=None\n","        \n","        nn.init.normal_(self.lin.weight,mean=0.0, std=0.01)\n","        nn.init.zeros_(self.bias)\n","    \n","    def forward(self,x):\n","        return self.lin(x)+self.bias.view(1, self.bias.shape[0])\n","    \n","        \n","    \n","class PixelNorm(nn.Module):\n","    def __init__(self):\n","        super(PixelNorm, self).__init__()\n","        self.epsilon = 1e-8\n","\n","    def forward(self, x):\n","        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n","\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","        super(ConvBlock, self).__init__()\n","        self.use_pn = use_pixelnorm\n","        self.conv1 = WSConv2d(in_channels, out_channels)\n","        self.conv2 = WSConv2d(out_channels, out_channels)\n","        self.leaky = nn.LeakyReLU(0.2)\n","        self.pn = PixelNorm()\n","\n","    def forward(self, x):\n","        x = self.leaky(self.conv1(x))\n","        x = self.pn(x) if self.use_pn else x\n","        x = self.leaky(self.conv2(x))\n","        x = self.pn(x) if self.use_pn else x\n","        return x\n","\n","class LinearBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","        super(LinearBlock,self).__init__()\n","        self.lin=WSLinear(in_channels,out_channels)\n","        self.relu=nn.ReLU()\n","        self.pn=PixelNorm()\n","        self.use_pn=use_pixelnorm\n","    \n","    def forward(self,x):\n","        x = self.relu(self.lin(x))\n","        x = self.pn(x) if self.use_pn else x\n","        return x\n","        \n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Generator, self).__init__()\n","\n","        # initial takes 1x1 -> 4x4\n","        self.initial = nn.Sequential(\n","            PixelNorm(),\n","            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","            nn.LeakyReLU(0.2),\n","            PixelNorm(),\n","        )\n","\n","        self.initial_rgb = WSConv2d(\n","            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.prog_blocks, self.rgb_layers = (\n","            nn.ModuleList([]),\n","            nn.ModuleList([self.initial_rgb]),\n","        )\n","\n","        for i in range(\n","            len(factors) - 1\n","        ):  \n","            conv_in_c = int(in_channels * factors[i])\n","            conv_out_c = int(in_channels * factors[i + 1])\n","            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","            self.rgb_layers.append(\n","                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","    def fade_in(self, alpha, upscaled, generated):\n","        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n","\n","    def forward(self, x, alpha, steps):\n","        out = self.initial(x)\n","\n","        if steps == 0:\n","            return self.initial_rgb(out)\n","\n","        for step in range(steps):\n","            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n","            out = self.prog_blocks[step](upscaled)\n","        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n","        final_out = self.rgb_layers[steps](out)\n","        return self.fade_in(alpha, final_upscaled, final_out)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, z_dim, in_channels, img_channels=3):\n","        super(Discriminator, self).__init__()\n","        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n","        self.leaky = nn.LeakyReLU(0.2)\n","\n","        for i in range(len(factors) - 1, 0, -1):\n","            conv_in = int(in_channels * factors[i])\n","            conv_out = int(in_channels * factors[i - 1])\n","            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n","            self.rgb_layers.append(\n","                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n","            )\n","\n","        self.initial_rgb = WSConv2d(\n","            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n","        )\n","        self.rgb_layers.append(self.initial_rgb)\n","        self.avg_pool = nn.AvgPool2d(\n","            kernel_size=2, stride=2\n","        )  \n","        self.final_block = nn.Sequential(\n","           \n","            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n","            nn.LeakyReLU(0.2),\n","            WSConv2d(\n","                in_channels, 1, kernel_size=1, padding=0, stride=1\n","            ),  \n","        )\n","\n","    def fade_in(self, alpha, downscaled, out):\n","        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n","        return alpha * out + (1 - alpha) * downscaled\n","\n","    def minibatch_std(self, x):\n","        batch_statistics = (\n","            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n","        )\n","\n","        return torch.cat([x, batch_statistics], dim=1)\n","\n","    def forward(self, x, alpha, steps):\n","\n","        cur_step = len(self.prog_blocks) - steps\n","        out = self.leaky(self.rgb_layers[cur_step](x))\n","\n","        if steps == 0:  # i.e, image is 4x4\n","            out = self.minibatch_std(out)\n","            return self.final_block(out).view(out.shape[0], -1)\n","\n","        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n","        out = self.avg_pool(self.prog_blocks[cur_step](out))\n","\n","     \n","        out = self.fade_in(alpha, downscaled, out)\n","\n","        for step in range(cur_step + 1, len(self.prog_blocks)):\n","            out = self.prog_blocks[step](out)\n","            out = self.avg_pool(out)\n","\n","        out = self.minibatch_std(out)\n","        return self.final_block(out).view(out.shape[0], -1)\n","\n","\n","class VariationalMiner(nn.Module):\n","    def __init__(self, z_dim):\n","        super(VariationalMiner, self).__init__()\n","        self.l_block1= LinearBlock(z_dim, z_dim*2, use_pixelnorm=True)\n","        self.l_block2= LinearBlock(z_dim*2, z_dim*2, use_pixelnorm=True)\n","        self.l_block3= LinearBlock(z_dim*2, z_dim, use_pixelnorm=True)\n","        self.fc_mu = LinearBlock(z_dim, z_dim, use_pixelnorm=False)  # Mean\n","        self.fc_logvar = LinearBlock(z_dim, z_dim, use_pixelnorm=False)  # Log-variance\n","    \n","    def reparameterize(self, mu, logvar):\n","        # Reparameterization trick\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)  # Random noise\n","        return mu + eps * std\n","    \n","    def forward(self, x):\n","        x=self.l_block1(x)\n","        x=self.l_block2(x)\n","        x=self.l_block3(x)\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        \n","        # Sample latent vector using reparameterization trick\n","        z = self.reparameterize(mu, logvar)\n","        \n","         # Reshape for generator input\n","        z = torch.unsqueeze(z, -1)\n","        z = torch.unsqueeze(z, -1)\n","        \n","        return z, mu, logvar\n","\n","        \n","        \n","Z_DIM = 256\n","IN_CHANNELS = 256\n","gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n","critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n","miner=VariationalMiner(Z_DIM)\n","\n","for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n","    num_steps = int(log2(img_size / 4))\n","    x = torch.randn((1, Z_DIM))\n","    y,_,_ = miner(x)\n","    z = gen(y, 0.5, steps=num_steps)\n","    assert z.shape == (1, 3, img_size, img_size)\n","    out = critic(z, alpha=0.5, steps=num_steps)\n","    assert out.shape == (1, 1)\n","    print(f\"Success! At img size: {img_size}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Helper functions"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-04T06:38:51.048115Z","iopub.status.busy":"2024-10-04T06:38:51.047610Z","iopub.status.idle":"2024-10-04T06:38:59.225394Z","shell.execute_reply":"2024-10-04T06:38:59.224410Z","shell.execute_reply.started":"2024-10-04T06:38:51.048079Z"},"trusted":true},"outputs":[],"source":["\n","def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","    \n","def plot_images(images, epoch):\n","    images = images.cpu().detach().numpy().transpose(0, 2, 3, 1)\n","    fig, axes = plt.subplots(1, len(images), figsize=(15, 15))\n","    for i, img in enumerate(images):\n","        axes[i].imshow(img)\n","        axes[i].axis(\"off\")\n","    plt.show()\n","\n","import os\n","import shutil\n","\n","def save_real_images(loader, output_dir=\"real_images\", num_images=500):\n","    \"\"\"Save real images from DataLoader to a specified folder.\"\"\"\n","    shutil.rmtree(output_dir, ignore_errors=True) \n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    num_saved = 0\n","    for i, (real_images, _) in enumerate(loader):\n","        for j, real_image in enumerate(real_images):\n","            if num_saved >= num_images:\n","                return\n","            save_image(real_image * 0.5 + 0.5, f\"{output_dir}/real_{num_saved}.png\")\n","            num_saved += 1\n","            \n","def save_fake_images(output_dir=\"fake_images\", num_images=500):\n","    \"\"\"Generate and save fake images from the generator to a specified folder.\"\"\"\n","\n","    shutil.rmtree(output_dir, ignore_errors=True)    \n","    os.makedirs(output_dir, exist_ok=True)\n","    \n","    \n","    gen = Generator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n","    gen_checkpoint = torch.load(\"/kaggle/working/generator_finetuned_st2.pth\")\n","    gen.load_state_dict(gen_checkpoint[\"state_dict\"])\n","    gen.eval()\n","    \n","    miner= VariationalMiner(config.Z_DIM).to(config.DEVICE)\n","    miner_checkpoint = torch.load(\"/kaggle/working/miner_finetuned_st2.pth\")\n","    miner.load_state_dict(miner_checkpoint[\"state_dict\"])\n","    miner.eval()\n","\n","    with torch.no_grad():\n","            NOISE = torch.randn(1, config.Z_DIM).to(config.DEVICE)\n","            _, mu, logvar = miner(NOISE)      \n","            std = torch.exp(0.5 * logvar)\n","        for i in range(num_images):\n","            epsilon = torch.randn_like(std)\n","            z = mu + epsilon * std\n","            z_reshaped = z.view(-1, config.IN_CHANNELS, 1, 1) \n","            image_fakes = gen(z_reshaped, alpha=1, steps=config.STEP) * 0.5 + 0.5  # Scale to [0, 1]  \n","#             print(image_fakes.shape)\n","            save_image(image_fakes, f\"{output_dir}/fake_{i}.png\")"]},{"cell_type":"markdown","metadata":{},"source":["### Configuration"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:39:03.284756Z","iopub.status.busy":"2024-10-04T06:39:03.284007Z","iopub.status.idle":"2024-10-04T06:39:03.945515Z","shell.execute_reply":"2024-10-04T06:39:03.944494Z","shell.execute_reply.started":"2024-10-04T06:39:03.284705Z"},"trusted":true},"outputs":[],"source":["import cv2\n","import torch\n","from math import log2\n","\n","class configuration():\n","    def __init__(self):\n","        self.STEP = 4\n","        self.DATASET = '/kaggle/input/shape-dataset'\n","        self.DATASET_SAMPLE=800\n","        self.CHECKPOINT_GEN = \"/kaggle/input/progan_multi/pytorch/default/1/generator.pth\"\n","        self.CHECKPOINT_CRITIC = \"/kaggle/input/progan_multi/pytorch/default/1/critic.pth\"\n","        self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.IMAGE_SIZE = 64\n","        self.LEARNING_RATE = 1e-3\n","        self.BATCH_SIZE = 32\n","        self.CHANNELS_IMG = 3\n","        self.Z_DIM = 256  \n","        self.IN_CHANNELS = 256 \n","        self.CRITIC_ITERATIONS = 1\n","        self.LAMBDA_GP = 10\n","        self.EPOCHS = 500\n","        self.FIXED_NOISE = torch.randn(8, self.Z_DIM).to(self.DEVICE)\n","        self.NUM_WORKERS = 4\n","        self.MINER_LAYERS=4\n","config=configuration()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:39:03.947714Z","iopub.status.busy":"2024-10-04T06:39:03.947367Z","iopub.status.idle":"2024-10-04T06:39:03.956319Z","shell.execute_reply":"2024-10-04T06:39:03.955390Z","shell.execute_reply.started":"2024-10-04T06:39:03.947681Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","from math import log2\n","from tqdm import tqdm\n","\n","torch.backends.cudnn.benchmarks = True\n","\n","\n","\n","def get_loader(image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                [0.5 for _ in range(config.CHANNELS_IMG)],\n","                [0.5 for _ in range(config.CHANNELS_IMG)],\n","            ),\n","        ]\n","    )\n","    batch_size = config.BATCH_SIZE\n","    \n","    dataset = datasets.ImageFolder(root=config.DATASET, transform=transform)\n","    \n","    indices = random.sample(range(len(dataset)), config.DATASET_SAMPLE)\n","    dataset = Subset(dataset, indices)\n","    print(len(dataset))\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=config.NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    return loader, dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Training Setup"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:39:32.614371Z","iopub.status.busy":"2024-10-04T06:39:32.613690Z","iopub.status.idle":"2024-10-04T06:39:32.624942Z","shell.execute_reply":"2024-10-04T06:39:32.624014Z","shell.execute_reply.started":"2024-10-04T06:39:32.614330Z"},"trusted":true},"outputs":[],"source":["def train_fn_stage1(\n","    critic,\n","    gen,\n","    miner,\n","    loader,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_miner,\n","    scaler_miner,\n","    scaler_critic,\n","):\n","    '''In this stage the miner network is trained keeping the generator weights frozen'''\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(config.DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        noise = torch.randn(cur_batch_size, config.Z_DIM).to(config.DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            z, _, _ = miner(noise)\n","            fake = gen(z, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=config.DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + config.LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_miner.zero_grad()\n","        scaler_miner.scale(loss_gen).backward()\n","        scaler_miner.step(opt_miner)\n","        scaler_miner.update()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:39:32.984310Z","iopub.status.busy":"2024-10-04T06:39:32.983709Z","iopub.status.idle":"2024-10-04T06:39:32.993901Z","shell.execute_reply":"2024-10-04T06:39:32.993097Z","shell.execute_reply.started":"2024-10-04T06:39:32.984276Z"},"trusted":true},"outputs":[],"source":["def train_fn_stage2(\n","    critic,\n","    gen,\n","    miner,\n","    loader,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","    scaler_gen,\n","    scaler_critic,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(config.DEVICE)\n","        cur_batch_size = real.shape[0]\n","        \n","        noise = torch.randn(cur_batch_size, config.Z_DIM).to(config.DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            z, _, _ = miner(noise)\n","            fake = gen(z, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=config.DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + config.LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T06:39:45.433005Z","iopub.status.busy":"2024-10-04T06:39:45.432108Z"},"trusted":true},"outputs":[],"source":["import shutil\n","def main(): \n","    shutil.rmtree(\"/kaggle/working/\", ignore_errors=True)\n","    gen = Generator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n","    critic = Discriminator(config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG).to(config.DEVICE)\n","    miner = VariationalMiner(config.Z_DIM).to(config.DEVICE)\n","    \n","    gen_checkpoint = torch.load(config.CHECKPOINT_GEN)\n","    gen.load_state_dict(gen_checkpoint[\"state_dict\"])\n","\n","    critic_checkpoint = torch.load(config.CHECKPOINT_CRITIC)\n","    critic.load_state_dict(critic_checkpoint[\"state_dict\"])\n","\n","    # initialize optimizers and scalers\n","    opt_gen = optim.Adam(list(gen.parameters())+list(miner.parameters()), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n","    opt_critic = optim.Adam(critic.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n","    opt_miner = optim.Adam(miner.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n","    \n","    scaler_critic = torch.cuda.amp.GradScaler()\n","    scaler_gen = torch.cuda.amp.GradScaler()\n","    scaler_miner = torch.cuda.amp.GradScaler()\n","\n","    # start at step that corresponds to img size that we set in config\n","    step = config.STEP\n","    alpha = 1\n","\n","    loader, dataset = get_loader(image_size=config.IMAGE_SIZE)  # Load real images\n","\n","    # Save real images to the \"real_images\" folder for evaluation purpose\n","    save_real_images(loader, output_dir=\"real_images\", num_images=config.DATASET_SAMPLE)\n","    print(f'real_images_saved: {len(os.listdir(\"real_images\"))}')\n","    \n","    print(\"STARTING STAGE 1\")\n","\n","    for epoch in range(100):\n","        print(f\"Epoch [{epoch+1}/{100}]\")\n","        critic.train()\n","        miner.train()\n","        gen.eval() \n","        train_fn_stage1(\n","            critic,\n","            gen,\n","            miner,\n","            loader,\n","            step,\n","            alpha,\n","            opt_critic,\n","            opt_miner,\n","            scaler_miner,\n","            scaler_critic,\n","        )\n","    \n","\n","        if epoch % 20 == 0:\n","            miner.eval()\n","            with torch.no_grad():\n","                noise= torch.randn(8, config.Z_DIM).to(config.DEVICE)\n","                x,_,_=miner(noise)\n","                fixed_fakes = gen(x, alpha, step) * 0.5 + 0.5  # Scale to [0, 1]\n","                plot_images(fixed_fakes, epoch)\n","            miner.train()\n","            \n","    save_checkpoint(gen, opt_gen, filename=\"generator_finetuned_st1.pth\")\n","    save_checkpoint(critic, opt_critic, filename=\"critic_finetuned_st1.pth\")\n","    save_checkpoint(miner, opt_miner, filename=\"miner_finetuned_st1.pth\")\n","    print(\"STARTING STAGE 2\")        \n","    for epoch in range(500):\n","        print(f\"Epoch [{epoch+1}/{500}]\")\n","        critic.train()\n","        miner.train()\n","        gen.train()\n","\n","        train_fn_stage2(\n","                    critic,\n","                    gen,\n","                    miner,\n","                    loader,\n","                    step,\n","                    alpha,\n","                    opt_critic,\n","                    opt_gen,\n","                    scaler_gen,\n","                    scaler_critic,\n","                )\n","        \n","        \n","        if epoch % 20 == 0:\n","            miner.eval()\n","            gen.eval()\n","            with torch.no_grad():\n","                noise= torch.randn(8, config.Z_DIM).to(config.DEVICE)\n","                x,_,_=miner(noise)\n","                fixed_fakes = gen(x, alpha, step) * 0.5 + 0.5  # Scale to [0, 1]\n","                plot_images(fixed_fakes, epoch)\n","            miner.train()\n","            gen.train()\n","        save_checkpoint(gen, opt_gen, filename=\"generator_finetuned_st2.pth\")\n","        save_checkpoint(critic, opt_critic, filename=\"critic_finetuned_st2.pth\")\n","        save_checkpoint(miner, opt_miner, filename=\"miner_finetuned_st2.pth\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip -q install torch-fidelity"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch_fidelity import calculate_metrics\n","\n","def calculate_fid(real_images_dir, fake_images_dir):\n","    metrics = calculate_metrics(\n","        input1=real_images_dir,  # Path to the directory with real images\n","        input2=fake_images_dir,  # Path to the directory with generated images\n","        cuda=True,               # Use GPU if availabl\n","        isc=True, fid=True\n","    )\n","    \n","    return metrics\n","\n","#inference\n","save_fake_images(\"fake_images\", 1000)\n","print(f'num_images: {len(os.listdir(\"fake_images\"))}')\n","metrics=calculate_fid(\"/kaggle/working/fake_images\",\"/kaggle/working/real_images\")"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize the distribution after Mining"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-24T17:27:55.943309Z","iopub.status.busy":"2024-09-24T17:27:55.942919Z","iopub.status.idle":"2024-09-24T17:28:10.007725Z","shell.execute_reply":"2024-09-24T17:28:10.006835Z","shell.execute_reply.started":"2024-09-24T17:27:55.943271Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","\n","z_dim = 256  # The dimension of latent space\n","batch_size = 2000  # Number of points want to visualize\n","noise = torch.randn((batch_size, z_dim))\n","\n","\n","miner.eval() \n","with torch.no_grad():\n","    _, mu, logvar = miner(noise)\n","    \n","    std = torch.exp(0.5 * logvar)\n","    \n","    epsilon = torch.randn_like(std)\n","    \n","    z = mu + epsilon * std\n","    z_reshaped = z.view(-1, config.IN_CHANNELS, 1, 1) \n","\n","# Using t-SNE for 2D visualization\n","tsne = TSNE(n_components=2, random_state=42)\n","latent_2d = tsne.fit_transform(z_reshaped)\n","\n","\n","# Plot the 2D latent space\n","plt.figure(figsize=(8, 6))\n","plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c='blue', alpha=0.6)\n","plt.title(\"t-SNE Visualization of Latent Space After Passing Through Miner\")\n","plt.xlabel(\"Dimension 1\")\n","plt.ylabel(\"Dimension 2\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5328092,"sourceId":8919705,"sourceType":"datasetVersion"},{"datasetId":5755626,"sourceId":9465881,"sourceType":"datasetVersion"},{"datasetId":5758735,"sourceId":9471256,"sourceType":"datasetVersion"},{"modelId":119808,"modelInstanceId":95616,"sourceId":113917,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
